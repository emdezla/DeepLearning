#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri May 15 16:15:21 2020

@author: SrAlejandro
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import prologuefunctions as prologue #I have done a python path




###############################Import Data############################


N = 1000
train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(N)



##############################First Model#########################################
#Test error rate 19%
#Convergence 20 epochs

class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        
        self.conv1 = nn.Conv2d(2, 32, kernel_size = 3)
        self.bn1 = nn.BatchNorm2d(32)
        self.conv2 = nn.Conv2d(32, 64, kernel_size = 3)
        self.bn2 = nn.BatchNorm2d(64)
        self.conv3 = nn.Conv2d(64, 128, kernel_size = 3)
        self.bn3 = nn.BatchNorm2d(128)
        
        self.fc1 = nn.Linear(128, 100)
        self.out = nn.Linear(100, 2)
    
    def forward(self, x):
        
        x = F.relu(self.conv1(x))
        x = self.bn1(x)
        
        x = F.relu(F.max_pool2d(self.conv2(x), stride = 2, kernel_size = 2))
        x = self.bn2(x)
        
        x = F.relu(F.max_pool2d(self.conv3(x), stride = 2, kernel_size = 2))
        x = self.bn3(x)
        
        x = F.relu(self.fc1(x.view(-1,128)))
        x = self.out(x)
        
        return x
    

###############################Second Model#####################################
# Error rate 13-14%
# Convergence fast
# Computationally more expensive 




class NetWeightSharing(nn.Module):
    def __init__(self):
        super(NetWeightSharing, self).__init__()
        
        self.conv1 = nn.Conv2d(1, 32, kernel_size = 3)
        self.bn1 = nn.BatchNorm2d(32)
        self.conv2 = nn.Conv2d(32, 64, kernel_size = 3)
        self.bn2 = nn.BatchNorm2d(64)
        self.conv3 = nn.Conv2d(64, 128, kernel_size = 3)
        self.bn3 = nn.BatchNorm2d(128)
        
        self.fc1 = nn.Linear(128, 100)
        
        self.out = nn.Linear(200, 2)
    
    def sharing_layers(self, x):
        x = F.relu(self.conv1(x))
        x = self.bn1(x)
        
        x = F.relu(F.max_pool2d(self.conv2(x), stride = 2, kernel_size = 2))
        x = self.bn2(x)
        
        x = F.relu(F.max_pool2d(self.conv3(x), stride = 2, kernel_size = 2))
        x = self.bn3(x)
        x = F.relu(self.fc1(x.view(-1,128)))
        return x
        
    def forward(self, x):
        
        x1 = self.sharing_layers(x[:,0].unsqueeze(1))
        x2 = self.sharing_layers(x[:,1].unsqueeze(1))
        
        output = torch.cat((x1,x2),1)
        output = self.out(output)
        
        return output
        
        
        
model = NetWeightSharing()

#####################################Third Model###########################################
#Error rate 10.9%




class NetAux_Share(nn.Module):
    def __init__(self):
        super(NetAux_Share, self).__init__()
        
        self.conv1 = nn.Conv2d(1, 32, kernel_size = 3)
        self.bn1 = nn.BatchNorm2d(32)
        self.conv2 = nn.Conv2d(32, 64, kernel_size = 3)
        self.bn2 = nn.BatchNorm2d(64)
        self.conv3 = nn.Conv2d(64, 128, kernel_size = 3)
        self.bn3 = nn.BatchNorm2d(128)
        
        self.fc1 = nn.Linear(128, 100)
        
        self.out1 = nn.Linear(100, 10)
        self.out2 = nn.Linear(100, 10)
        
        self.out = nn.Linear(20, 2)
    
    def sharing_layers(self, x):
        x = F.relu(self.conv1(x))
        x = self.bn1(x)
        
        x = F.relu(F.max_pool2d(self.conv2(x), stride = 2, kernel_size = 2))
        x = self.bn2(x)
        
        x = F.relu(F.max_pool2d(self.conv3(x), stride = 2, kernel_size = 2))
        x = self.bn3(x)
        x = F.relu(self.fc1(x.view(-1,128)))
        return x
        
    def forward(self, x):
        
        x1 = self.sharing_layers(x[:,0].unsqueeze(1))
        x2 = self.sharing_layers(x[:,1].unsqueeze(1))
        
        x1 = self.out1(x1)
        x2 = self.out2(x2)
        
        output = torch.cat((x1,x2),1)
        
        output = self.out(output)
        
        return output, x1, x2





################################Training Function#################################3
def training_model(train_input, train_target, model, batch, lr):
    optimizer = torch.optim.SGD(model.parameters(), lr = lr)
    criterion = nn.CrossEntropyLoss()
    total_loss = 0
    for b in range(0, train_input.size(0), batch):
        output = model(train_input.narrow(0, b, batch))            
        loss = criterion(output, train_target.narrow(0, b, batch))
        model.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss 


def compute_nb_errors(model, data_input, data_target, batch):
    
    nb_data_errors = 0

    for b in range(0, data_input.size(0), batch):
        output = model(data_input.narrow(0, b, batch))
        _, predicted_classes = torch.max(output, 1)
        for k in range(batch):
            if data_target[b + k] != predicted_classes[k]:
                nb_data_errors = nb_data_errors + 1
    return nb_data_errors

##########################Train Aux Function########################################
    
def training_aux(train_input, train_target, train_classes, model, batch, lr):
    optimizer = torch.optim.SGD(model.parameters(), lr = lr)
    Binary_Criterion = nn.CrossEntropyLoss()
    Aux_Criterion = nn.CrossEntropyLoss()
    total_loss_aux = 0
    total_loss_bin = 0
    final_total_loss = 0
    for b in range(0, train_input.size(0), batch):
        output, aux1, aux2 = model(train_input.narrow(0, b, batch))
        target_classes = train_classes.narrow(0, b, batch)
        target_comparison = train_target.narrow(0, b, batch)
        aux_loss = Aux_Criterion(aux1, target_classes[:,0]) + Aux_Criterion(aux2, target_classes[:,1])
        binary_loss = Binary_Criterion(output, target_comparison)
        final_loss = 0.7*binary_loss + 0.3*aux_loss
        model.zero_grad()
        final_loss.backward()
        optimizer.step()
        total_loss_aux += aux_loss
        total_loss_bin += binary_loss
        final_total_loss += final_loss
    return final_total_loss, total_loss_aux, total_loss_bin

def compute_nb_errors_aux(model, data_input, data_target, batch):
    
    nb_data_errors = 0

    for b in range(0, data_input.size(0), batch):
        output,_,_ = model(data_input.narrow(0, b, batch))
        _, predicted_classes = torch.max(output, 1)
        for k in range(batch):
            if data_target[b + k] != predicted_classes[k]:
                nb_data_errors = nb_data_errors + 1
    return nb_data_errors



#############################Training Part###############################
epochs = 25
batch = 100
#model = SimpleNet()
sum(p.numel() for p in model.parameters() if p.requires_grad)
for e in range(epochs):
    model.train()
    training_model(train_input, train_target, model, batch, 0.1)
    model.eval()
    print(compute_nb_errors(model, train_input,train_target, 100), " ",compute_nb_errors(model, test_input,test_target, 100))


##################################################################

    
epochs = 25
batch = 100
model = NetAux_Share()
sum(p.numel() for p in model.parameters() if p.requires_grad)
for e in range(epochs):
    model.train()
    training_aux(train_input, train_target, train_classes, model, batch, 0.1)
    model.eval()
    print(compute_nb_errors_aux(model, train_input,train_target, 100), " ",compute_nb_errors_aux(model, test_input,test_target, 100))

    

        
        
        
        