{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "merged_files",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zI901RWuUhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7ucl_T5uwun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from itertools import accumulate\n",
        "import dlc_practical_prologue as prologue"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvcNe9-xTyWt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(0)\n",
        "N = 1000\n",
        "train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(N)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU6qDtOJT0OQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def k_fold(data, ind, k):\n",
        "    n = len(data)\n",
        "    return data[n * (ind-1)//k:n * ind//k], data[0:n * (ind-1)//k], data[n * ind//k:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPnkqEbwT2F5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "##############################First Model#########################################\n",
        "#Test error rate 19%\n",
        "#Convergence 20 epochs\n",
        "\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(2, 32, kernel_size = 3)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size = 3)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size = 3)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        \n",
        "        self.fc1 = nn.Linear(128, 100)\n",
        "        self.out = nn.Linear(100, 2)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.bn1(x)\n",
        "        \n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), stride = 2, kernel_size = 2))\n",
        "        x = self.bn2(x)\n",
        "        \n",
        "        x = F.relu(F.max_pool2d(self.conv3(x), stride = 2, kernel_size = 2))\n",
        "        x = self.bn3(x)\n",
        "        \n",
        "        x = F.relu(self.fc1(x.view(-1,128)))\n",
        "        x = self.out(x)\n",
        "        \n",
        "        return x\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwfqG8_1T4EN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "###############################Second Model#####################################\n",
        "# Error rate 13-14%\n",
        "# Convergence fast\n",
        "# Computationally more expensive \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class NetWeightSharing(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetWeightSharing, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size = 3)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size = 3)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size = 3)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        \n",
        "        self.fc1 = nn.Linear(128, 100)\n",
        "        \n",
        "        self.out = nn.Linear(200, 2)\n",
        "    \n",
        "    def sharing_layers(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.bn1(x)\n",
        "        \n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), stride = 2, kernel_size = 2))\n",
        "        x = self.bn2(x)\n",
        "        \n",
        "        x = F.relu(F.max_pool2d(self.conv3(x), stride = 2, kernel_size = 2))\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(self.fc1(x.view(-1,128)))\n",
        "        return x\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x1 = self.sharing_layers(x[:,0].unsqueeze(1))\n",
        "        x2 = self.sharing_layers(x[:,1].unsqueeze(1))\n",
        "        \n",
        "        output = torch.cat((x1,x2),1)\n",
        "        output = self.out(output)\n",
        "        \n",
        "        return output\n",
        "        \n",
        "        \n",
        "        \n",
        "model = NetWeightSharing()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LMYbHqET5sd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "#####################################Third Model###########################################\n",
        "#Error rate 10.9%\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class NetAux_Share(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetAux_Share, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size = 3)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size = 3)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size = 3)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        \n",
        "        self.fc1 = nn.Linear(128, 100)\n",
        "        \n",
        "        self.out1 = nn.Linear(100, 10)\n",
        "        self.out2 = nn.Linear(100, 10)\n",
        "        \n",
        "        self.out = nn.Linear(20, 2)\n",
        "    \n",
        "    def sharing_layers(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.bn1(x)\n",
        "        \n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), stride = 2, kernel_size = 2))\n",
        "        x = self.bn2(x)\n",
        "        \n",
        "        x = F.relu(F.max_pool2d(self.conv3(x), stride = 2, kernel_size = 2))\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(self.fc1(x.view(-1,128)))\n",
        "        return x\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x1 = self.sharing_layers(x[:,0].unsqueeze(1))\n",
        "        x2 = self.sharing_layers(x[:,1].unsqueeze(1))\n",
        "        \n",
        "        x1 = self.out1(x1)\n",
        "        x2 = self.out2(x2)\n",
        "        \n",
        "        output = torch.cat((x1,x2),1)\n",
        "        \n",
        "        output = self.out(output)\n",
        "        \n",
        "        return output, x1, x2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWXq1WuAT7nD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "################################FULLY CONNECTED NETWORK#################################\n",
        "\n",
        "class SimpleFCNetwork(nn.Module):\n",
        "    def __init__(self, h_shapes):\n",
        "       \n",
        "        super().__init__()\n",
        "        self.length = len(h_shapes)\n",
        "        self.hidden_l = nn.ModuleList()\n",
        "        for i in range(len(h_shapes)-1):\n",
        "            self.hidden_l.append(nn.Linear(h_shapes[i], h_shapes[i+1]))\n",
        "        \n",
        "        \n",
        "        self.fc1 = nn.Linear(392, h_shapes[0])\n",
        "        self.fc3 = nn.Linear(h_shapes[self.length-1], 2)\n",
        "       \n",
        "        \n",
        "    def forward(self, x):\n",
        "      \n",
        "  \n",
        "        x = F.relu(self.fc1(x))\n",
        "     \n",
        "        for i in range(self.length-1):\n",
        "          x = F.relu(self.hidden_l[i](x))\n",
        "      \n",
        "        x = F.relu(self.fc3(x))\n",
        "       \n",
        "        return x\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxyyviA7UIJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################FULLY CONNECTED NETWORK WITH AUX LOSS#################################\n",
        "\n",
        "\n",
        "class FCAux(nn.Module):\n",
        "    def __init__(self, h_shapes):\n",
        "       \n",
        "        super().__init__()\n",
        "        self.length = len(h_shapes)\n",
        "        self.hidden_l = nn.ModuleList()\n",
        "        for i in range(len(h_shapes)-1):\n",
        "            self.hidden_l.append(nn.Linear(h_shapes[i], h_shapes[i+1]))\n",
        "        \n",
        "       \n",
        "        self.fc1 = nn.Linear(392, h_shapes[0])\n",
        "        self.fc3 = nn.Linear(h_shapes[self.length-1], 2)\n",
        "        self.fc4 = nn.Linear(392, 20)\n",
        "        \n",
        "    def forward(self, x):\n",
        "  \n",
        "        y = x\n",
        "        x = F.relu(self.fc1(x))\n",
        "        y = F.relu(self.fc4(y))\n",
        "        for i in range(self.length-1):\n",
        "          x = F.relu(self.hidden_l[i](x))\n",
        "        y = y.view(-1,2,10)\n",
        "        x = F.relu(self.fc3(x))\n",
        "       \n",
        "        return x,y\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJoZHC2vUPbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "################################PRECISION-RECALL CURVE#################################\n",
        "def precision_recall(target, pred):\n",
        "  \n",
        "  target = [True if x else False for x in target]\n",
        "  sorted_ind = sorted(range(len(list(pred))), key=lambda k: list(pred)[k])\n",
        "  sorted_ind.reverse()\n",
        "\n",
        "  pred = [pred[i] for i in sorted_ind]\n",
        "\n",
        "  target = [target[i] for i in sorted_ind]\n",
        "\n",
        "  uniqs = list(set(list(pred)))\n",
        "  uniqs.sort(reverse=True)\n",
        "  \n",
        "  indexes = [list(pred).index(x) for x in uniqs]\n",
        "\n",
        "  t = [1 if x else 0 for x in target]\n",
        "  tp_accum = list(accumulate(t))\n",
        "  \n",
        "  \n",
        "  tps = [tp_accum[i] for i in indexes]\n",
        "\n",
        "  ones = [1]*(len(target))\n",
        "  fp_accum = list(accumulate([a_i - b_i for a_i, b_i in zip(ones, t)]))\n",
        "  fps = [1+t-tp for t,tp in zip(indexes,tps)]\n",
        "\n",
        "  \n",
        "  precision = [x/(x + y) if x+y else 0 for x,y in zip(tps,fps)]\n",
        "  recall =  [x/tps[-1] if tps[-1] else 0 for x in tps]\n",
        "\n",
        "  thresholds = [pred[i] for i in indexes]\n",
        "  \n",
        "  last_ind = len(tps)-2\n",
        "  sl = slice(last_ind, None, -1)\n",
        "\n",
        "  pre = precision[sl]\n",
        "  pre.append(1.0)\n",
        "  rec = recall[sl]\n",
        "  rec.append(0.0)\n",
        "  return pre, rec, thresholds[sl]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch4v0KM1URRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################ROC CURVE#################################\n",
        "\n",
        "def roc_true(target, pred):\n",
        "  \n",
        "  target = [True if x else False for x in target]\n",
        "  sorted_ind = sorted(range(len(list(pred))), key=lambda k: list(pred)[k])\n",
        "  sorted_ind.reverse()\n",
        " \n",
        "  pred = [pred[i] for i in sorted_ind]\n",
        "\n",
        "  target = [target[i] for i in sorted_ind]\n",
        "\n",
        "  uniqs = list(set(list(pred)))\n",
        "  uniqs.sort(reverse=True)\n",
        "  \n",
        "  indexes = [list(pred).index(x) for x in uniqs]\n",
        "  \n",
        "\n",
        "  t = [1 if x else 0 for x in target]\n",
        "  tp_accum = list(accumulate(t))\n",
        "  \n",
        "  \n",
        "  tps = [tp_accum[i] for i in indexes]\n",
        "\n",
        "  ones = [1]*(len(target))\n",
        "  fp_accum = list(accumulate([a_i - b_i for a_i, b_i in zip(ones, t)]))\n",
        "  fps = [1+t-tp for t,tp in zip(indexes,tps)]\n",
        "\n",
        "  thresholds = [pred[i] for i in indexes]\n",
        "\n",
        "  thresholds.insert(0, thresholds[0] + 1)\n",
        "  tps.insert(0, 0)\n",
        "  fps.insert(0, 0)\n",
        "  fpr = [i/fps[-1] for i in fps]\n",
        "  tpr = [i/tps[-1] for i in tps]\n",
        "\n",
        "  return fpr, tpr, thresholds\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE1LmTKWUWIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "################################Training Function#################################\n",
        "def training_model(train_input, train_target, model, batch, lr):\n",
        "    model.to(device)\n",
        "    train_input.to(device)\n",
        "    train_target.to(device)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    total_loss = 0\n",
        "    for b in range(0, train_input.size(0), batch):\n",
        "        output = model(train_input.narrow(0, b, batch).to(device))            \n",
        "        loss = criterion(output, train_target.narrow(0, b, batch).to(device))\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss \n",
        "\n",
        "\n",
        "def compute_nb_errors(model, data_input, data_target, batch):\n",
        "    model.to(device)\n",
        "    data_input.to(device)\n",
        "    data_target.to(device)\n",
        "    nb_data_errors = 0\n",
        "\n",
        "    for b in range(0, data_input.size(0), batch):\n",
        "        output = model(data_input.narrow(0, b, batch).to(device))\n",
        "        _, predicted_classes = torch.max(output, 1)\n",
        "        for k in range(batch):\n",
        "            if data_target[b + k] != predicted_classes[k]:\n",
        "                nb_data_errors = nb_data_errors + 1\n",
        "    return nb_data_errors\n",
        "\n",
        "##########################Train Aux Function########################################\n",
        "    \n",
        "def training_aux(CNNFlag, train_input, train_target, train_classes, model, batch, lr):\n",
        "    model.to(device)\n",
        "    train_input.to(device)\n",
        "    train_target.to(device)\n",
        "    train_classes.to(device)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
        "    Binary_Criterion = nn.CrossEntropyLoss()\n",
        "    Aux_Criterion = nn.CrossEntropyLoss()\n",
        "    total_loss_aux = 0\n",
        "    total_loss_bin = 0\n",
        "    final_total_loss = 0\n",
        "    for b in range(0, train_input.size(0), batch):\n",
        "      if CNNFlag:\n",
        "        output, aux1, aux2 = model(train_input.narrow(0, b, batch).to(device))\n",
        "      else:\n",
        "        output, aux = model(train_input.narrow(0, b, batch).to(device))\n",
        "      target_classes = train_classes.narrow(0, b, batch).to(device)\n",
        "      target_comparison = train_target.narrow(0, b, batch).to(device)\n",
        "      if CNNFlag:\n",
        "        aux_loss = Aux_Criterion(aux1, target_classes[:,0].to(device)) + Aux_Criterion(aux2, target_classes[:,1].to(device))\n",
        "      else:\n",
        "        aux_loss = Aux_Criterion(aux[:,0], target_classes[:,0].to(device)) + Aux_Criterion(aux[:,0], target_classes[:,1].to(device))\n",
        "      binary_loss = Binary_Criterion(output, target_comparison)\n",
        "      final_loss = 0.7*binary_loss + 0.3*aux_loss\n",
        "      model.zero_grad()\n",
        "      final_loss.backward()\n",
        "      optimizer.step()\n",
        "      total_loss_aux += aux_loss\n",
        "      total_loss_bin += binary_loss\n",
        "      final_total_loss += final_loss\n",
        "    return final_total_loss, total_loss_aux, total_loss_bin\n",
        "\n",
        "def compute_nb_errors_aux(CNNFlag, model, data_input, data_target, batch):\n",
        "    model.to(device)\n",
        "    data_input.to(device)\n",
        "    data_target.to(device)\n",
        "    \n",
        "    nb_data_errors = 0\n",
        "\n",
        "    for b in range(0, data_input.size(0), batch):\n",
        "      if CNNFlag:\n",
        "        output,_,_ = model(data_input.narrow(0, b, batch).to(device))\n",
        "      else: \n",
        "         output,_ = model(data_input.narrow(0, b, batch).to(device))\n",
        "      _, predicted_classes = torch.max(output, 1)\n",
        "      for k in range(batch):\n",
        "          if data_target[b + k] != predicted_classes[k]:\n",
        "              nb_data_errors = nb_data_errors + 1\n",
        "    return nb_data_errors\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xfpn1wcZ-BB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_until_convergence(CNNFlag, AuxFlag, model, training_data, training_target, training_classes, batch, l):\n",
        "  prev_loss = 999999\n",
        "  #TRAIN UNTIL CINVERGENCE\n",
        "  epochs = 0\n",
        "  while(True):\n",
        "    epochs += 1\n",
        "    if not AuxFlag:\n",
        "      cur_loss = training_model(training_data, training_target, model, batch, l)\n",
        "    else:\n",
        "      cur_loss, _, _ = training_aux(CNNFlag, training_data, training_target, training_classes, model, batch, l)\n",
        "    \n",
        "    \n",
        "    if(abs(prev_loss - cur_loss) <= 0.001):\n",
        "      break\n",
        "    prev_loss = cur_loss\n",
        "  return epochs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAJrmSViaQuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def report_error(CNNFlag, AuxFlag, model, test_data, test_target, batch):\n",
        "  error = 0\n",
        "  if not AuxFlag: \n",
        "    error = compute_nb_errors(model, test_data,test_target, batch)\n",
        "  else:\n",
        "    error = compute_nb_errors_aux(CNNFlag, model, test_data,test_target, batch)\n",
        "  return error\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGliPciAUXws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################HYPERPARAMETER TUNING WITH CROSSVALIDATION#################################\n",
        "def cross_validation_tuning(CNNFlag, AuxFlag,train_data, train_target, training_classes, k, lr, batches, neurons = None):\n",
        "  err_min = 999999\n",
        "\n",
        "  #TUNING NUMBER OF BATCHES\n",
        "  for batch in batches:\n",
        "    #TUNING LEARNING RATE\n",
        "    for l in lr:\n",
        "      #CHECKING IF TUNING IS FIR A CNN NETWORK\n",
        "      if(not CNNFlag):\n",
        "        #TUNING FOR NUMBER OF HIDDEN LAYERS FOR FC NETWORK\n",
        "        for neuron_count in neurons:\n",
        "          error = 0\n",
        "          #CROSS VALIDATION\n",
        "          for i in range(1, k + 1): \n",
        "            if not AuxFlag:\n",
        "              model = SimpleFCNetwork(neuron_count)\n",
        "            else:\n",
        "              model = FCAux(neuron_count)\n",
        "            #FOLDING THE DATA\n",
        "            test_data, train_1_data, train_2_data = k_fold(train_data, i, k)\n",
        "            test_target, train_1_target, train_2_target = k_fold(train_target, i, k)\n",
        "            training_data = torch.cat((train_1_data,train_2_data),0)\n",
        "            training_target = torch.cat((train_1_target,train_2_target),0)\n",
        "            prev_loss = 999999\n",
        "            #TRAIN UNTIL CINVERGENCE\n",
        "            train_until_convergence(CNNFlag, AuxFlag, model, training_data, training_target, train_classes, batch, l)\n",
        "\n",
        "            error += report_error(CNNFlag, AuxFlag, model, test_data, test_target, batch)\n",
        "          error /= k\n",
        "          \n",
        "          if error < err_min:\n",
        "            err_min = error\n",
        "            l_min = l\n",
        "            batch_min = batch\n",
        "            neruron_min =  neuron_count if neurons else None\n",
        "      #NUMBER OF HIDDEN LAYERS IS NOT TUNED FOR CNN\n",
        "      else:\n",
        "       \n",
        "        error = 0\n",
        "        #CROSS VALIDATION\n",
        "        for i in range(1, k + 1): \n",
        "          #FOLDING THE DATA\n",
        "          test_data, train_1_data, train_2_data = k_fold(train_data, i, k)\n",
        "          test_target, train_1_target, train_2_target = k_fold(train_target, i, k)\n",
        "          training_data = torch.cat((train_1_data,train_2_data),0)\n",
        "          training_target = torch.cat((train_1_target,train_2_target),0)\n",
        "          if not AuxFlag:\n",
        "            model = NetWeightSharing()\n",
        "          else:\n",
        "            model = NetAux_Share()\n",
        "          prev_loss = 999999\n",
        "          #TRAIN UNTIL CONVERGENCE\n",
        "          train_until_convergence(CNNFlag, AuxFlag, model, training_data, training_target, train_classes, batch, l)\n",
        " \n",
        "\n",
        "          error += report_error(CNNFlag, AuxFlag, model, test_data, test_target, batch)\n",
        "        error /= k\n",
        "\n",
        "       \n",
        "        #TAKE THE HYPERPARAMETERS WITH MINIMAL ERROR\n",
        "        if error < err_min:\n",
        "          err_min = error\n",
        "          l_min = l\n",
        "          batch_min = batch\n",
        "          neruron_min =  neuron_count if neurons else None\n",
        "  return err_min, l_min, batch_min, neruron_min\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oF3_7518gfs9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_predictions(CNNFlag, AuxFlag, model, test_input):\n",
        "  if not AuxFlag:\n",
        "    output = model(test_input.to(device))\n",
        "  elif CNNFlag:\n",
        "    output, _, _ = model(test_input.to(device))\n",
        "  else:\n",
        "    output, _ = model(test_input.to(device))\n",
        "\n",
        "  m = nn.Softmax()\n",
        "  probs = m(output)\n",
        "  return probs[:, 1]\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZZZBoNqePIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_roc_curve(test_target, predictions):\n",
        "  fpr, tpr, _  = (roc_true(test_target, predictions))\n",
        "  plt.plot(fpr, tpr, marker='.', label='mlp')\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  # show the legend\n",
        "  plt.legend()\n",
        "  # show the plot\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS_bscfWfqpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def  plot_precision_recall_curve(test_target, predictions):\n",
        "  lr_recall, lr_precision, _  = (precision_recall(test_target, predictions))\n",
        "  plt.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
        "  # axis labels\n",
        "  plt.xlabel('Recall')\n",
        "  plt.ylabel('Precision')\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcubmjdZi9f0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "45b42cb2-90e4-497d-c1ff-9c9df8020654"
      },
      "source": [
        "err, otpimum_lr, batch_size, neuron_size = cross_validation_tuning(True, False, train_input, train_target, train_classes,10,[0.1,0.05,0.01],[20, 100, 50])\n",
        "print(err, otpimum_lr, batch_size, neuron_size)"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-146-f30d318bed3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0motpimum_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneuron_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0motpimum_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneuron_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-137-77ec6ca36ddd>\u001b[0m in \u001b[0;36mcross_validation_tuning\u001b[0;34m(CNNFlag, AuxFlag, train_data, train_target, training_classes, k, lr, batches, neurons)\u001b[0m\n\u001b[1;32m     52\u001b[0m           \u001b[0mprev_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m999999\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m           \u001b[0;31m#TRAIN UNTIL CONVERGENCE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m           \u001b[0mtrain_until_convergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCNNFlag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAuxFlag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-135-caf568ee1ef7>\u001b[0m in \u001b[0;36mtrain_until_convergence\u001b[0;34m(CNNFlag, AuxFlag, model, training_data, training_target, training_classes, batch, l)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mAuxFlag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m       \u001b[0mcur_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mcur_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_aux\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCNNFlag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-134-d3fcdab6e76e>\u001b[0m in \u001b[0;36mtraining_model\u001b[0;34m(train_input, train_target, model, batch, lr)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDOjIp_-dtrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#############################Training Part###############################\n",
        "\n",
        "model = NetWeightSharing()\n",
        "number_of_epochs = train_until_convergence(True, False, model, train_input, train_target, train_classes, batch_size, otpimum_lr)\n",
        "print(\"number of epochs until convergence\", number_of_epochs)\n",
        "\n",
        "predictions = get_predictions(True, False,model, test_input)\n",
        "\n",
        "plot_precision_recall_curve(test_target.tolist(), torch.flatten(predictions).tolist())\n",
        "plot_roc_curve(test_target.tolist(), torch.flatten(predictions).tolist())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfmVlMYdjITR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "5f54ad60-7895-4cfe-d01e-d9d2240980bb"
      },
      "source": [
        "err, otpimum_lr, batch_size, neuron_size = cross_validation_tuning(True, True, train_input, train_target, train_classes,10,[0.1,0.05, 0.01],[20, 100, 50])\n",
        "print(err, otpimum_lr, batch_size, neuron_size)"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-147-6694ab7f1ab1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0motpimum_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneuron_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0motpimum_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneuron_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-137-77ec6ca36ddd>\u001b[0m in \u001b[0;36mcross_validation_tuning\u001b[0;34m(CNNFlag, AuxFlag, train_data, train_target, training_classes, k, lr, batches, neurons)\u001b[0m\n\u001b[1;32m     52\u001b[0m           \u001b[0mprev_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m999999\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m           \u001b[0;31m#TRAIN UNTIL CONVERGENCE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m           \u001b[0mtrain_until_convergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCNNFlag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAuxFlag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-135-caf568ee1ef7>\u001b[0m in \u001b[0;36mtrain_until_convergence\u001b[0;34m(CNNFlag, AuxFlag, model, training_data, training_target, training_classes, batch, l)\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mcur_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       \u001b[0mcur_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_aux\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCNNFlag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-134-d3fcdab6e76e>\u001b[0m in \u001b[0;36mtraining_aux\u001b[0;34m(CNNFlag, train_input, train_target, train_classes, model, batch, lr)\u001b[0m\n\u001b[1;32m     60\u001b[0m       \u001b[0mfinal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbinary_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0maux_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m       \u001b[0mfinal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m       \u001b[0mtotal_loss_aux\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maux_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKerpnEwidnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = NetAux_Share()\n",
        "number_of_epochs = train_until_convergence(True, True, model, train_input, train_target, train_classes, batch_size, otpimum_lr)\n",
        "print(\"number of epochs until convergence\", number_of_epochs)\n",
        "\n",
        "predictions = get_predictions(True, True, model,test_input)\n",
        "\n",
        "plot_precision_recall_curve(test_target.tolist(), torch.flatten(predictions).tolist())\n",
        "plot_roc_curve(test_target.tolist(), torch.flatten(predictions).tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f26nAEn-jSVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_input_first_approach =  torch.zeros(size = (1000,392))\n",
        "test_input_first_approach = torch.zeros(size = (1000,392))\n",
        "\n",
        "for i in range(train_input.size(0)):\n",
        "  \n",
        "    train_input_first_approach[i]  = torch.cat((torch.flatten(train_input[i,0]),torch.flatten(train_input[i,1])),0)\n",
        "    test_input_first_approach[i] = torch.cat((torch.flatten(test_input[i,0]),torch.flatten(test_input[i,1])),0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgP-DddKcLDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "err, otpimum_lr, batch_size, neuron_size  = cross_validation_tuning(False, False, train_input_first_approach, train_target, train_classes,10,[0.1,0.005],[100, 50],[[200,50,10],[150,100],[100]])\n",
        "print(err,otpimum_lr, batch_size, neuron_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNMfPXh_j3sj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = SimpleFCNetwork(neuron_size)\n",
        "number_of_epochs = train_until_convergence(False, False, model, train_input_first_approach, train_target, train_classes, batch_size, otpimum_lr)\n",
        "print(\"number of epochs until convergence\", number_of_epochs)\n",
        "\n",
        "predictions = get_predictions(False, False, model, test_input_first_approach)\n",
        "\n",
        "plot_precision_recall_curve(test_target.tolist(), torch.flatten(predictions).tolist())\n",
        "plot_roc_curve(test_target.tolist(), torch.flatten(predictions).tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzO84TZrmLSe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "err, otpimum_lr, batch_size, neuron_size  = cross_validation_tuning(False, True, train_input_first_approach, train_target, train_classes,10,[0.1,0.005],[100, 50],[[200,50,10],[150,100],[100]])\n",
        "print(err,otpimum_lr, batch_size, neuron_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2PugUhFnRDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = FCAux(neuron_size)\n",
        "number_of_epochs = train_until_convergence(False, True, model, train_input_first_approach, train_target, train_classes, batch_size, otpimum_lr)\n",
        "print(\"number of epochs until convergence\", number_of_epochs)\n",
        "\n",
        "predictions = get_predictions(False, True, model, test_input_first_approach)\n",
        "\n",
        "plot_precision_recall_curve(test_target.tolist(), torch.flatten(predictions).tolist())\n",
        "plot_roc_curve(test_target.tolist(), torch.flatten(predictions).tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K92k0NB4og8a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}